{"id":"c19aa74e-6da8-4479-912e-13362d813b7e","data":{"nodes":[{"id":"PDFReader-caVH2","type":"genericNode","position":{"x":-665.9239555288782,"y":-11.725620861354429},"data":{"type":"PDFReader","node":{"template":{"_type":"Component","pdf":{"trace_as_metadata":true,"file_path":"","fileTypes":["pdf"],"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"pdf","display_name":"PDF","advanced":false,"dynamic":false,"info":"Supported file types: PDF","title_case":false,"type":"file","_input_type":"FileInput","load_from_db":false},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.schema.document import Document\nfrom typing import List\nimport uuid\n\nclass PDFReaderComponent(Component):\n    display_name = \"PDF Reader\"\n    description = \"A generic PDF loader.\"\n    icon = \"file-text\"\n    name = \"PDFReader\"\n\n    inputs = [\n        FileInput(\n            name=\"pdf\",\n            display_name=\"PDF\",\n            file_types=[\"pdf\"],\n            info=f\"Supported file types: PDF\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Splits\", name=\"splits\", method=\"build_output\"),\n    ]\n        \n    def _splits_to_data(self, splits):\n        return [Data(text=split, data={\"id\": str(uuid.uuid4()), \"source\": self.pdf}) for split in splits]\n    \n    def build_output(self) -> List[Data]:\n     loader = PyPDFLoader(self.pdf)\n     pages = loader.load()\n     docs = []\n     for p in pages:\n         content = p.page_content.strip().lower()\n         if content:\n             doc = \" \".join(content.split())\n             if len(doc) > 50:\n                docs.append(Document(page_content=doc, metadata=p.metadata))\n    \n    \n     r_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=500,\n     )\n     \n     splits = r_splitter.split_documents(docs)\n     \n     data = []\n     \n     for s in splits:\n         doc = next(doc for doc in docs if doc.metadata[\"page\"] == s.metadata[\"page\"])\n         data.append(Data(\n             text = doc.page_content, \n             chunk= s.page_content, \n             page = s.metadata[\"page\"],\n             source = s.metadata[\"source\"],\n             id = str(uuid.uuid4())\n            )\n        )\n    \n    #  data = []\n    #  for i in iterator:\n    #     doc = i.page_content.strip().lower()\n    #     if doc:\n    #          doc = \" \".join(doc.split())\n    #          if len(doc) > 50:\n    #             data.append(doc)\n                \n    #  text = \"\".join(data)\n     \n    #  r_splitter = RecursiveCharacterTextSplitter(\n    #     chunk_size=1000,\n    #     chunk_overlap=500,\n    #  )\n     \n    #  splits = r_splitter.split_text(text)\n     \n    #  data = self._splits_to_data(splits)\n     \n     self.status = data\n     \n     return data","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false}},"description":"A generic PDF loader.","icon":"file-text","base_classes":["Data"],"display_name":"PDF Reader","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"splits","display_name":"Splits","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["pdf"],"beta":false,"edited":true},"id":"PDFReader-caVH2","description":"A generic PDF loader.","display_name":"PDF Reader"},"selected":false,"width":384,"height":309,"dragging":false,"positionAbsolute":{"x":-665.9239555288782,"y":-11.725620861354429}},{"id":"NEO4J-kpw7I","type":"genericNode","position":{"x":-176.57815617902958,"y":-71.68291377672011},"data":{"type":"NEO4J","node":{"template":{"_type":"Component","splits":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"splits","display_name":"Splits","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import HandleInput, Output\nfrom langflow.schema import Data\nfrom langchain_community.graphs import Neo4jGraph\n\nclass NEO4JComponent(Component):\n    display_name = \"NEO4J\"\n    description = \"Knowledge Graphs for RAG.\"\n    icon = \"git-graph\"\n    name = \"NEO4J\"\n\n    inputs = [\n         HandleInput(\n            name=\"splits\",\n            display_name=\"Splits\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        DropdownInput(\n            name=\"openai_model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=[\n                \"text-embedding-3-small\",\n                \"text-embedding-3-large\",\n                \"text-embedding-ada-002\",\n            ],\n            value=\"text-embedding-3-small\",\n        ),\n        StrInput(name=\"openai_endpoint\", display_name=\"OpenAI Embeddings Endpoint\"),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\"),\n        StrInput(name=\"neo4j_uri\", display_name=\"NEO4J URI\"),\n        StrInput(name=\"neo4j_username\", display_name=\"NEO4J USERNAME\"),\n        StrInput(name=\"neo4j_password\", display_name=\"NEO4J PASSWORD\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_chunks(self, splits):\n        parents = {}\n        chunks = []\n        chunk_seq_id = 0\n        for split in splits:\n            chunk_seq_id += 1\n            chunks.append({\n                \"chunkId\": split.id,\n                \"text\": split.chunk,\n                \"source\": split.source,\n                \"pageId\": split.page,\n                \"chunkSeqId\": chunk_seq_id\n            })\n            \n            if split.page not in parents:\n                parents[split.page] = {\"text\": split.text, \"pageId\": split.page}\n                \n        return chunks, parents\n        \n    def build_output(self) -> Data:\n        kg = Neo4jGraph(\n            url=self.neo4j_uri, \n            username=self.neo4j_username, \n            password=self.neo4j_password\n        )\n        \n        chunks, parents = self.build_chunks(self.splits)\n        \n        ## CLEAR DATABASE\n    \n        kg.query(\"\"\" MATCH (n) DETACH DELETE n \"\"\")\n        kg.query(\"\"\" DROP CONSTRAINT unique_chunk IF EXISTS \"\"\")\n        kg.query(\"\"\" DROP INDEX unique_chunk IF EXISTS \"\"\")\n        kg.query(\"\"\" DROP INDEX chunks_embedding IF EXISTS \"\"\")\n        \n    \n        ## CREATE PARENT NODES \n        \n        merge_parent_node = \"\"\"\n        MERGE(p:Page {pageId: $params.pageId})\n        ON CREATE SET\n            p.text = $params.text\n        RETURN p\n        \"\"\"\n        node_count = 0\n        for p in parents:\n            kg.query(merge_parent_node, params={'params': parents[p]})\n            node_count += 1\n        \n        print(f\"Created {node_count} parent nodes\")\n        \n        ## CREATE NODE WITH PROPERTIES\n\n        merge_chunk_node = \"\"\"\n        MERGE(c:Chunk {chunkId: $params.chunkId})\n        ON CREATE SET \n            c.text = $params.text,\n            c.source = $params.source,\n            c.pageId = $params.pageId,\n            c.chunkSeqId = $params.chunkSeqId\n        RETURN c\n        \"\"\"\n        \n        node_count = 0\n        for chunk in chunks:\n            print(f\"Creating `:Chunk` node for chunk ID {chunk['chunkId']}\")\n            kg.query(merge_chunk_node, params={'params': chunk})\n            node_count += 1\n            \n        print(f\"Created {node_count} chunk nodes\")\n        \n        ## CREATE UNIQUE INDEX \n        \n        kg.query(\"\"\"\n        CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n        FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n        \"\"\")\n        \n        ## CREATE VECTOR INDEX\n\n        kg.query(\"\"\"\n         CREATE VECTOR INDEX `chunks_embedding` IF NOT EXISTS\n          FOR (c:Chunk) ON (c.textEmbedding) \n          OPTIONS { indexConfig: {\n            `vector.dimensions`: 1536,\n            `vector.similarity_function`: 'cosine'    \n         }}\"\"\")\n\n        ## DISPLAY INDEXES \n        # print(kg.query(\"SHOW INDEXES\"))\n        \n        kg.query(\"\"\"\n        MATCH (chunk:Chunk) WHERE chunk.textEmbedding IS NULL\n        WITH chunk, genai.vector.encode(\n            chunk.text, \n            \"OpenAI\", \n              {\n                token: $openAiApiKey, \n                endpoint: $openAiEndpoint,\n                model: $openAiModel\n              }) AS vector\n            CALL db.create.setNodeVectorProperty(chunk, \"textEmbedding\", vector)\n        \"\"\", params={\n                \"openAiApiKey\":self.openai_api_key, \n                \"openAiEndpoint\": self.openai_endpoint,\n                \"openAiModel\": self.openai_model,\n        })\n        \n        return Data(text=f\"Created {node_count} nodes\")\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"neo4j_password":{"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"value":"NEO4J_PASS","name":"neo4j_password","display_name":"NEO4J PASSWORD","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"neo4j_uri":{"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"value":"NEO4J_URI","name":"neo4j_uri","display_name":"NEO4J URI","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"neo4j_username":{"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"value":"NEO4J_USER","name":"neo4j_username","display_name":"NEO4J USERNAME","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"openai_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"openai_endpoint":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"https://api.openai.com/v1/embeddings","name":"openai_endpoint","display_name":"OpenAI Embeddings Endpoint","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"openai_model":{"combobox":false,"trace_as_metadata":true,"options":["text-embedding-3-small","text-embedding-3-large","text-embedding-ada-002"],"required":false,"placeholder":"","show":true,"value":"text-embedding-3-small","name":"openai_model","display_name":"Model","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"}},"description":"Knowledge Graphs for RAG.","icon":"git-graph","base_classes":["Data"],"display_name":"Neo4J Ingestion","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["splits","openai_model","openai_endpoint","openai_api_key","neo4j_uri","neo4j_username","neo4j_password"],"beta":false,"edited":true},"id":"NEO4J-kpw7I","description":"Knowledge Graphs for RAG.","display_name":"Neo4J Ingestion"},"selected":false,"width":384,"height":803,"dragging":false,"positionAbsolute":{"x":-176.57815617902958,"y":-71.68291377672011}},{"id":"NEO4J-c3EdX","type":"genericNode","position":{"x":-186.0189213893285,"y":-1904.949167512508},"data":{"type":"NEO4J","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nfrom langchain_community.vectorstores import Neo4jVector\nfrom langchain_community.graphs import Neo4jGraph\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains import RetrievalQAWithSourcesChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom typing import List\nimport functools\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nclass NEO4JComponent(Component):\n    display_name = \"NEO4J\"\n    description = \"Knowledge Graphs for RAG.\"\n    icon = \"git-graph\"\n    name = \"NEO4J\"\n\n    inputs = [\n        MultilineInput(\n            name=\"context\",\n            display_name=\"Context\"\n        ),\n        MultilineInput(\n            name=\"question\",\n            display_name=\"Question\"\n        ),\n        DropdownInput(\n            name=\"openai_embedding_model\",\n            display_name=\"Embedding Model\",\n            advanced=False,\n            options=[\n                \"text-embedding-3-small\",\n            ],\n            value=\"text-embedding-3-small\",\n        ),\n        DropdownInput(\n            name=\"openai_base_model\",\n            display_name=\"Base Model\",\n            advanced=False,\n            options=[\n                \"gpt-3.5-turbo\",\n                \"gpt-4o-mini\",\n                \"gpt-4o-mini-2024-07-18\",\n            ],\n            value=\"gpt-4o-mini-2024-07-18\",\n        ),\n        StrInput(name=\"openai_endpoint\", display_name=\"OpenAI Embeddings Endpoint\"),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\"),\n        StrInput(name=\"neo4j_uri\", display_name=\"NEO4J URI\"),\n        StrInput(name=\"neo4j_username\", display_name=\"NEO4J USERNAME\"),\n        StrInput(name=\"neo4j_password\", display_name=\"NEO4J PASSWORD\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n        Output(display_name=\"Search Results\", name=\"search_result\", method=\"build_search_result\"),\n    ]\n  \n    # def preprocess_question(self) -> str:\n    #     stop_words = set(stopwords.words('portuguese'))\n    #     question = self.question.lower()\n    #     words = word_tokenize(question)\n    #     filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n    #     processed_question = \" \".join(filtered_words)\n        \n    #     return processed_question\n\n    def prettychain(self, chain) -> str:\n        response = chain({\"question\": self.question.lower()}, return_only_outputs=True,)\n        return response['answer']\n    \n    def _build_prompt(self) -> PromptTemplate:\n        # template = \"\"\"\n        # <instrucoes>\n        # Você é um assistente especializado em respostas precisas sobre o livro Cosmos de Carl Sagan.\n        \n        # Regra Principal: A resposta deve ser clara, direta e concisa, evitando detalhes desnecessários.\n        \n        # Regras adicionais:\n        \n        # 1. Contexto: Utilize o campo Contexto abaixo como referência, não crie conteúdo fora deste escopo.\n        \n        # 2. Fidelidade ao Conteúdo: A resposta deve conter similaridade contextual com o texto original.\n        \n        # 3. Estilo e Tom: O estilo e o tom da resposta devem ser consistentes com os do texto original.\n        \n        # 4. Precisão: A resposta deve ser precisa e factual.\n        \n        # 5. Coerência e Coesão: As frases e parágrafos da resposta devem fluir de maneira lógica e coerente.\n        \n        # 6. Português do Brasil: A resposta deve ser sempre em Português do Brasil.\n        # </instrucoes>\n        \n        # Contexto:\n        # {summaries}\n        \n        # Pergunta:\n        # {question}\n        \n        # Resposta:\n        # \"\"\"\n        \n        template = \"\"\"\n        <instrucoes>\n        Você é um assistente especializado em respostas precisas sobre o livro Cosmos de Carl Sagan.\n        \n        Regra principal: A resposta deve ser completa e precisa.\n        \n        Regras Adicionais:\n        \n        1. Contexto: Utilize o campo Contexto abaixo como referência, não crie conteúdo fora deste escopo.\n        \n        2. Fidelidade ao Conteúdo: A resposta deve conter similaridade contextual com o texto original.\n        \n        3. Estilo e Tom: O estilo e o tom da resposta devem ser consistentes com os do texto original.\n        \n        4. Clareza e Concisão: A resposta deve ser clara, direta e concisa, evitando detalhes desnecessários.\n        \n        5. Coerência e Coesão: As frases e parágrafos da resposta devem fluir de maneira lógica e coerente.\n        \n        6. Português do Brasil: A resposta deve ser sempre em Português do Brasil.\n        </instrucoes>\n        \n        Contexto:\n        {summaries}\n        \n        Pergunta:\n        {question}\n        \n        Resposta:\n        \"\"\"\n        \n        return PromptTemplate(template=template)\n        \n    def _build_retriever(self, query):\n        VECTOR_INDEX_NAME = \"chunks_embedding\"\n        VECTOR_NODE_LABEL = \"Chunk\"\n        VECTOR_SOURCE_PROPERTY = \"text\"\n        \n        vector_store_window = Neo4jVector.from_existing_index(\n            embedding=OpenAIEmbeddings(openai_api_key=self.openai_api_key, model=self.openai_embedding_model),\n            url=self.neo4j_uri,\n            username=self.neo4j_username,\n            password=self.neo4j_password,\n            index_name=VECTOR_INDEX_NAME,\n            node_label=VECTOR_NODE_LABEL,\n            text_node_property=VECTOR_SOURCE_PROPERTY,\n            retrieval_query=query,\n        )\n        \n        return vector_store_window.as_retriever()\n        \n    def _build_chain(self, retriever):\n        prompt = self._build_prompt()\n    \n        chain_type_kwargs = {\"prompt\": prompt}\n        \n        chain = RetrievalQAWithSourcesChain.from_chain_type(\n            ChatOpenAI(temperature=0, api_key=self.openai_api_key, model=self.openai_base_model), \n            chain_type=\"stuff\",\n            chain_type_kwargs=chain_type_kwargs,\n            retriever=retriever\n        )\n        \n        return chain\n        \n    def build_search_result(self) -> List[Data]:\n        VECTOR_INDEX_NAME = \"chunks_embedding\"\n        \n        kg = Neo4jGraph(\n            url=self.neo4j_uri, \n            username=self.neo4j_username, \n            password=self.neo4j_password\n        )\n        \n        vector_search_query = \"\"\"\n        WITH genai.vector.encode(\n            $question, \n            \"OpenAI\", \n            {\n                token: $openAiApiKey, \n                endpoint: $openAiEndpoint,\n                model: $openAiModel\n            }) AS vector\n        CALL db.index.vector.queryNodes($index_name, $top_k, vector) yield node, score\n        RETURN score, node.text AS text\n        \"\"\"\n        \n        answers = kg.query(vector_search_query, \n                         params={\n                          \"question\": self.question.lower(), \n                          \"index_name\": VECTOR_INDEX_NAME, \n                          \"top_k\": 4,\n                          \"openAiApiKey\":self.openai_api_key, \n                          \"openAiEndpoint\": self.openai_endpoint,\n                          \"openAiModel\": self.openai_embedding_model\n                         })\n        \n        return [Data(text=answer[\"text\"], score=answer[\"score\"]) for answer in answers]\n        \n    def build_output(self) -> Message:\n        # neo4j_vector_store = Neo4jVector.from_existing_graph(\n        #     embedding=OpenAIEmbeddings(openai_api_key=self.openai_api_key, model=self.openai_model),\n        #     url=self.neo4j_uri,\n        #     username=self.neo4j_username,\n        #     password=self.neo4j_password,\n        #     index_name=VECTOR_INDEX_NAME,\n        #     node_label=VECTOR_NODE_LABEL,\n        #     text_node_properties=[VECTOR_SOURCE_PROPERTY],\n        #     embedding_node_property=VECTOR_EMBEDDING_PROPERTY,\n        # )\n\n        # retriever = neo4j_vector_store.as_retriever()\n        \n        retrieval_query_window = \"\"\"\n        MATCH window=\n            (:Chunk)-[:NEXT*0..1]->(node)-[:NEXT*0..1]->(:Chunk)\n        WITH node, score, window as longestWindow \n            ORDER BY length(window) DESC LIMIT 1\n        WITH nodes(longestWindow) as chunkList, node, score\n            MATCH (chunk:Chunk)-[:BELONGS_TO]->(page:Page)\n            WHERE chunk IN chunkList\n        WITH DISTINCT page, node, score\n            RETURN apoc.text.join(collect(page.text), \" \\n \") AS text,\n                score,\n                node {.source} AS metadata\n        \"\"\"\n        \n        # retrieval_query_window = \"\"\"\n        # MATCH window=\n        #     (:Chunk)-[:NEXT*0..1]->(node)-[:NEXT*0..1]->(:Chunk)\n        # WITH node, score, window as longestWindow \n        #   ORDER BY length(window) DESC LIMIT 1\n        # WITH nodes(longestWindow) as chunkList, node, score\n        #   UNWIND chunkList as chunkRows\n        # WITH collect(chunkRows.text) as textList, node, score\n        # RETURN apoc.text.join(textList, \" \\n \") as text,\n        #     score,\n        #     node {.source} AS metadata\n        # \"\"\"\n        \n        \n        retriever = self._build_retriever(retrieval_query_window)\n        chain = self._build_chain(retriever)\n        result = self.prettychain(chain)\n        \n        message = Message(\n            text=result\n        )\n        \n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"context":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"context","display_name":"Context","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"neo4j_password":{"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"value":"NEO4J_PASS","name":"neo4j_password","display_name":"NEO4J PASSWORD","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"neo4j_uri":{"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"value":"NEO4J_URI","name":"neo4j_uri","display_name":"NEO4J URI","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"neo4j_username":{"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"value":"NEO4J_USER","name":"neo4j_username","display_name":"NEO4J USERNAME","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"openai_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"openai_base_model":{"combobox":false,"trace_as_metadata":true,"options":["gpt-3.5-turbo","gpt-4o-mini","gpt-4o-mini-2024-07-18"],"required":false,"placeholder":"","show":true,"value":"gpt-4o-mini-2024-07-18","name":"openai_base_model","display_name":"Base Model","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_embedding_model":{"combobox":false,"trace_as_metadata":true,"options":["text-embedding-3-small"],"required":false,"placeholder":"","show":true,"value":"text-embedding-3-small","name":"openai_embedding_model","display_name":"Embedding Model","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_endpoint":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"https://api.openai.com/v1/embeddings","name":"openai_endpoint","display_name":"OpenAI Embeddings Endpoint","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"question":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"question","display_name":"Question","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Knowledge Graphs for RAG.","icon":"git-graph","base_classes":["Data","Message"],"display_name":"Neo4J","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true},{"types":["Data"],"selected":"Data","name":"search_result","display_name":"Search Results","method":"build_search_result","value":"__UNDEFINED__","cache":true}],"field_order":["context","question","openai_embedding_model","openai_base_model","openai_endpoint","openai_api_key","neo4j_uri","neo4j_username","neo4j_password"],"beta":false,"edited":true},"id":"NEO4J-c3EdX","description":"Knowledge Graphs for RAG.","display_name":"Neo4J"},"selected":false,"width":384,"height":1077,"positionAbsolute":{"x":-186.0189213893285,"y":-1904.949167512508},"dragging":false},{"id":"ChatOutput-HWwQE","type":"genericNode","position":{"x":360.5478921758789,"y":-1169.1014939679505},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"should_store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-HWwQE","description":"Display a chat message in the Playground.","display_name":"Chat Output"},"selected":false,"width":384,"height":317,"positionAbsolute":{"x":360.5478921758789,"y":-1169.1014939679505},"dragging":false},{"id":"ChatInput-0Q78M","type":"genericNode","position":{"x":-744.7262635409737,"y":-747.48199974542},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"files","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.memory import store_message\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Question\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Question\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"o que Kepler acreditava sobre a geometria em relação à criação do universo?","name":"input_value","display_name":"Question","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Question","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","sender","sender_name","session_id","files"],"beta":false,"edited":true},"id":"ChatInput-0Q78M","description":"Get chat inputs from the Playground.","display_name":"Chat Input"},"selected":false,"width":384,"height":317,"positionAbsolute":{"x":-744.7262635409737,"y":-747.48199974542},"dragging":false},{"id":"Memory-My7cj","type":"genericNode","position":{"x":-773.0439975245808,"y":-1381.8210746219083},"data":{"type":"Memory","node":{"template":{"_type":"Component","memory":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"memory","display_name":"External Memory","advanced":false,"input_types":["BaseChatMessageHistory"],"dynamic":false,"info":"Retrieve messages from an external memory. If empty, it will use the Langflow tables.","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs import HandleInput\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import get_messages, LCBuiltinChatMemory\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nfrom langflow.field_typing import BaseChatMemory\nfrom langchain.memory import ConversationBufferMemory\n\n\nclass MemoryComponent(Component):\n    display_name = \"Chat Memory\"\n    description = \"Retrieves stored chat messages from Langflow tables or an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    inputs = [\n        HandleInput(\n            name=\"memory\",\n            display_name=\"External Memory\",\n            input_types=[\"BaseChatMessageHistory\"],\n            info=\"Retrieve messages from an external memory. If empty, it will use the Langflow tables.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\", \"Machine and User\"],\n            value=\"Machine and User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"Session ID of the chat history.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Messages (Data)\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Messages (Text)\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n        Output(display_name=\"Memory\", name=\"lc_memory\", method=\"build_lc_memory\"),\n    ]\n\n    def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == \"Machine and User\":\n            sender = None\n\n        if self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = self.memory.messages\n            if sender:\n                expected_type = \"Machine\" if sender == \"Machine\" else \"User\"\n                stored = [m for m in stored if m.type == expected_type]\n            if order == \"ASC\":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n        else:\n            stored = get_messages(\n                sender=sender,\n                sender_name=sender_name,\n                session_id=session_id,\n                limit=n_messages,\n                order=order,\n            )\n        self.status = stored\n        return stored\n\n    def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n\n    def build_lc_memory(self) -> BaseChatMemory:\n        if self.memory:\n            chat_memory = self.memory\n        else:\n            chat_memory = LCBuiltinChatMemory(flow_id=self.graph.flow_id, session_id=self.session_id)\n        return ConversationBufferMemory(chat_memory=chat_memory)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"n_messages":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":100,"name":"n_messages","display_name":"Number of Messages","advanced":true,"dynamic":false,"info":"Number of messages to retrieve.","title_case":false,"type":"int"},"order":{"trace_as_metadata":true,"options":["Ascending","Descending"],"required":false,"placeholder":"","show":true,"value":"Ascending","name":"order","display_name":"Order","advanced":true,"dynamic":false,"info":"Order of the messages.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User","Machine and User"],"required":false,"placeholder":"","show":true,"value":"Machine and User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID of the chat history.","title_case":false,"type":"str"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{sender_name}: {text}","name":"template","display_name":"Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.","title_case":false,"type":"str"}},"description":"Retrieves stored chat messages from Langflow tables or an external memory.","icon":"message-square-more","base_classes":["BaseChatMemory","Data","Message"],"display_name":"Chat Memory","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"messages","display_name":"Messages (Data)","method":"retrieve_messages","value":"__UNDEFINED__","cache":true,"hidden":false},{"types":["Message"],"selected":"Message","name":"messages_text","display_name":"Messages (Text)","method":"retrieve_messages_as_text","value":"__UNDEFINED__","cache":true},{"types":["BaseChatMemory"],"selected":"BaseChatMemory","name":"lc_memory","display_name":"Memory","method":"build_lc_memory","value":"__UNDEFINED__","cache":true}],"field_order":["memory","sender","sender_name","n_messages","session_id","order","template"],"beta":false,"edited":false},"id":"Memory-My7cj","description":"Retrieves stored chat messages from Langflow tables or an external memory.","display_name":"Chat Memory"},"selected":false,"width":384,"height":419,"positionAbsolute":{"x":-773.0439975245808,"y":-1381.8210746219083},"dragging":false},{"id":"LangWatchEvaluatorComponent-MZkM1","type":"genericNode","position":{"x":910.1055404268336,"y":-788.0220064240813},"data":{"type":"LangWatchEvaluatorComponent","node":{"template":{"_type":"Component","context_data":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"context_data","display_name":"RAG Search Results (opcional)","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to be used as context for evaluation.","title_case":false,"type":"other"},"answer":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"answer","display_name":"Chat Output","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Conecte seu chat output.","title_case":false,"type":"str"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import re\r\nfrom langflow.custom import Component\r\nfrom langflow.inputs import MessageTextInput, DataInput\r\nfrom langflow.schema.message import Message\r\nfrom langflow.template import Output\r\nfrom langflow.schema import Data\r\nimport langwatch\r\nimport os\r\n\r\nos.environ[\"LANGWATCH_API_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0aW1lc3RhbXAiOjE3MjExNTE2NTY3MDQsInJhbmQiOjAuNjgzNzI5NDExMTcyMzYsImlhdCI6MTcyMTE1MTY1Nn0.IHNSVO1N2uaUjl5y2j_E0AwvuvFOwo5y56dpNg4QNBo\"\r\n\r\nclass LangWatchEvaluatorComponent(Component):\r\n    display_name = \"LangWatch Evaluator\"\r\n    description = \"Evaluates a question-answer pair using LangWatch and provides a trace URL.\"\r\n    icon = \"view\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"answer\",\r\n            display_name=\"Chat Output\",\r\n            info=\"Conecte seu chat output.\",\r\n        ),\r\n          MessageTextInput(\r\n            name=\"question\",\r\n            display_name=\"Chat Input\",\r\n            info=\"Conecte o seu chat input.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"ground_truth\",\r\n            display_name=\"Resposta Correta\",\r\n            info=\"A resposta correta fornecida.\",\r\n        ),\r\n        DataInput(\r\n            name=\"context_data\",\r\n            display_name=\"RAG Search Results (opcional)\",\r\n            info=\"The data to be used as context for evaluation.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"user_email\",\r\n            display_name=\"User Email\",\r\n            info=\"The user ID for the trace metadata.\",\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"user_name\",\r\n            display_name=\"Participant Name\",\r\n            info=\"Full name for identification in the trace metadata.\",\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"user_cpf\",\r\n            display_name=\"Participant CPF\",\r\n            info=\"CPF for identification in for the trace metadata.\",\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"question_id\",\r\n            display_name=\"Question ID\",\r\n            info=\"The question ID for the trace metadata.\",\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Veja o resultado >\", name=\"trace_url\", method=\"evaluate\"),\r\n    ]\r\n\r\n    async def evaluate(self) -> Message:\r\n        question = self.question\r\n        answer = self.answer\r\n        ground_truth = self.ground_truth\r\n        context_data = self.context_data\r\n        user_email = self.user_email if self.user_email else \"\"\r\n        question_id = self.question_id if self.question_id else \"\"\r\n        user_name = self.user_name if self.user_name else \"\"\r\n        user_cpf = self.user_cpf if self.user_cpf else \"\"\r\n\r\n        # Validate email if provided\r\n        if user_email and not self.validate_email(user_email):\r\n            raise ValueError(f\"Invalid email address: {user_email}\")\r\n\r\n        # Validate CPF if provided\r\n        if user_cpf and not self.validate_cpf(user_cpf):\r\n            raise ValueError(f\"Invalid CPF: {user_cpf}\")\r\n\r\n        langwatch.api_key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0aW1lc3RhbXAiOjE3MjExNTE2NTY3MDQsInJhbmQiOjAuNjgzNzI5NDExMTcyMzYsImlhdCI6MTcyMTE1MTY1Nn0.IHNSVO1N2uaUjl5y2j_E0AwvuvFOwo5y56dpNg4QNBo'\r\n\r\n        trace = langwatch.trace(\r\n            metadata={\r\n                \"user_id\": user_email,\r\n                \"question_id\": question_id,\r\n                \"user_name\": user_name,\r\n                \"user_cpf\": user_cpf\r\n            },\r\n            expected_output=ground_truth\r\n        )\r\n        \r\n        contexts = [item.text.replace(\"\\t\", \" \").replace(\"\\n\", \" \") for item in context_data[:5]] if context_data else []\r\n        rag_span = trace.span(type=\"rag\", name=\"LangWatch Evaluator\", input=question, contexts=contexts, output=answer)\r\n        rag_span.end()\r\n\r\n        trace.send_spans()\r\n\r\n        public_url = trace.share()\r\n        print(\"See the trace at:\", public_url)\r\n\r\n        self.status = f\"Enviado com sucesso. visite URL para ver o resultado: {public_url}\"\r\n        message = Message(text=public_url)\r\n        return message\r\n        \r\n    def validate_email(self, email):\r\n        pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\r\n        return re.match(pattern, email) is not None\r\n\r\n    def validate_cpf(self, cpf):\r\n        if not re.match(r'^(?!(\\d)\\1{10})\\d{9}[\\d]{2}$', cpf):\r\n            return False\r\n\r\n        total = sum(int(cpf[i]) * (10 - i) for i in range(9))\r\n        check1 = (total * 10 % 11) % 10\r\n\r\n        total = sum(int(cpf[i]) * (11 - i) for i in range(10))\r\n        check2 = (total * 10 % 11) % 10\r\n\r\n        return cpf[-2:] == f\"{check1}{check2}\"","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"ground_truth":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"Kepler acreditava que a geometria existiu antes da Criação e era coeterna com a Mente de Deus.","name":"ground_truth","display_name":"Resposta Correta","advanced":false,"input_types":["Message"],"dynamic":false,"info":"A resposta correta fornecida.","title_case":false,"type":"str"},"question":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"question","display_name":"Chat Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Conecte o seu chat input.","title_case":false,"type":"str"},"question_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"question_id","display_name":"Question ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The question ID for the trace metadata.","title_case":false,"type":"str"},"user_cpf":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"user_cpf","display_name":"Participant CPF","advanced":true,"input_types":["Message"],"dynamic":false,"info":"CPF for identification in for the trace metadata.","title_case":false,"type":"str"},"user_email":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"user_email","display_name":"User Email","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The user ID for the trace metadata.","title_case":false,"type":"str"},"user_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"user_name","display_name":"Participant Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Full name for identification in the trace metadata.","title_case":false,"type":"str"}},"description":"Evaluates a question-answer pair using LangWatch and provides a trace URL.","icon":"view","base_classes":["Message"],"display_name":"Langwatch Evaluator","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"trace_url","display_name":"Veja o resultado >","method":"evaluate","value":"__UNDEFINED__","cache":true}],"field_order":["answer","question","ground_truth","context_data","user_email","user_name","user_cpf","question_id"],"beta":false,"edited":true},"id":"LangWatchEvaluatorComponent-MZkM1","description":"Evaluates a question-answer pair using LangWatch and provides a trace URL.","display_name":"Langwatch Evaluator"},"selected":false,"width":384,"height":589,"positionAbsolute":{"x":910.1055404268336,"y":-788.0220064240813},"dragging":false},{"id":"CreateList-vx3Cq","type":"genericNode","position":{"x":-1250.0427595367464,"y":-764.955039523877},"data":{"type":"CreateList","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import StrInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\n\nclass CreateListComponent(Component):\n    display_name = \"Create List\"\n    description = \"Creates a list of texts.\"\n    icon = \"list\"\n    name = \"CreateList\"\n\n    inputs = [\n        StrInput(\n            name=\"texts\",\n            display_name=\"Texts\",\n            info=\"Enter one or more texts.\",\n            is_list=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Questions\", name=\"questions\", method=\"create_list\"),\n    ]\n\n    def create_list(self) -> list[Data]:\n        data = [Data(text=text) for text in self.texts]\n        self.status = data\n        return data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"texts":{"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["quantos cortes são necessários para dividir uma torta de maçã até chegar a um átomo?","o que mudou na percepção de Júpiter após o encontro da Voyager 1, segundo o autor?","qual é o principal risco de fingir irracionalidade para um blefe nuclear eficaz?","segundo Farrington, qual fator econômico levou ao declínio da ciência jônica?","qual foi o problema enfrentado pela Voyager 1 que poderia ter comprometido a coleta de dados científicos?","o que Kepler acreditava sobre a geometria em relação à criação do universo?","qual foi o impacto da rejeição da existência do éter luminífero\" na compreensão da propagação da luz?\"","Qual é a analogia usada no texto para descrever a percepção de objetos ao viajar próximo à velocidade da luz?","Qual é a opinião de Carl Sagan sobre a possibilidade de formas de vida baseadas em elementos diferentes do carbono e água?","qual é a visão perturbadora e instigante sobre a hierarquia de universos mencionada no texto?"],"name":"texts","display_name":"Texts","advanced":false,"dynamic":false,"info":"Enter one or more texts.","title_case":false,"type":"str"}},"description":"Creates a list of texts.","icon":"list","base_classes":["Data"],"display_name":"Questions","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"questions","display_name":"Questions","method":"create_list","value":"__UNDEFINED__","cache":true}],"field_order":["texts"],"beta":false,"edited":true},"id":"CreateList-vx3Cq","description":"Creates a list of texts.","display_name":"Questions"},"selected":false,"width":384,"height":759,"dragging":false,"positionAbsolute":{"x":-1250.0427595367464,"y":-764.955039523877}},{"id":"CreateList-eK7i2","type":"genericNode","position":{"x":1384.8121658871078,"y":-800.8772975513713},"data":{"type":"CreateList","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import StrInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\n\nclass CreateListComponent(Component):\n    display_name = \"Create List\"\n    description = \"Creates a list of texts.\"\n    icon = \"list\"\n    name = \"CreateList\"\n\n    inputs = [\n        StrInput(\n            name=\"texts\",\n            display_name=\"Texts\",\n            info=\"Enter one or more texts.\",\n            is_list=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Valid Answers\", name=\"valid_answers\", method=\"create_list\"),\n    ]\n\n    def create_list(self) -> list[Data]:\n        data = [Data(text=text) for text in self.texts]\n        self.status = data\n        return data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"texts":{"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["Cerca de noventa cortes.","Júpiter passou a ser visto como um lugar a ser explorado, não apenas um ponto de luz.","Acostumar-se com a irracionalidade e deixar de ser fingimento.","A economia escravista.","Problema no posicionamento da lança da plataforma de varredura científica.","Kepler acreditava que a geometria existiu antes da Criação e era coeterna com a Mente de Deus.","Mostrou que a luz viaja no vácuo, fundamentando a teoria da relatividade de Einstein.","Os objetos aparecem comprimidos numa pequena janela circular à sua frente.","Sagan acha a possibilidade interessante, mas prefere carbono e água pela abundância e adequação para a vida.","A visão é de uma hierarquia infinita de universos, onde cada partícula pode ser um universo, e nosso universo pode ser uma partícula em um universo maior."],"name":"texts","display_name":"Texts","advanced":false,"dynamic":false,"info":"Enter one or more texts.","title_case":false,"type":"str"}},"description":"Creates a list of texts.","icon":"list","base_classes":["Data"],"display_name":"Answers","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"valid_answers","display_name":"Valid Answers","method":"create_list","value":"__UNDEFINED__","cache":true}],"field_order":["texts"],"beta":false,"edited":true},"id":"CreateList-eK7i2","description":"Creates a list of texts.","display_name":"Answers"},"selected":false,"width":384,"height":759,"positionAbsolute":{"x":1384.8121658871078,"y":-800.8772975513713},"dragging":false}],"edges":[{"source":"ChatInput-0Q78M","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0Q78Mœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"NEO4J-c3EdX","targetHandle":"{œfieldNameœ:œquestionœ,œidœ:œNEO4J-c3EdXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"question","id":"NEO4J-c3EdX","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-0Q78M","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-0Q78M{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0Q78Mœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-NEO4J-c3EdX{œfieldNameœ:œquestionœ,œidœ:œNEO4J-c3EdXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"NEO4J-c3EdX","sourceHandle":"{œdataTypeœ:œNEO4Jœ,œidœ:œNEO4J-c3EdXœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-HWwQE","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-HWwQEœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-HWwQE","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"NEO4J","id":"NEO4J-c3EdX","name":"output","output_types":["Message"]}},"id":"reactflow__edge-NEO4J-c3EdX{œdataTypeœ:œNEO4Jœ,œidœ:œNEO4J-c3EdXœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-HWwQE{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-HWwQEœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"Memory-My7cj","sourceHandle":"{œdataTypeœ:œMemoryœ,œidœ:œMemory-My7cjœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}","target":"NEO4J-c3EdX","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œNEO4J-c3EdXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context","id":"NEO4J-c3EdX","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Memory","id":"Memory-My7cj","name":"messages_text","output_types":["Message"]}},"id":"reactflow__edge-Memory-My7cj{œdataTypeœ:œMemoryœ,œidœ:œMemory-My7cjœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}-NEO4J-c3EdX{œfieldNameœ:œcontextœ,œidœ:œNEO4J-c3EdXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"ChatOutput-HWwQE","sourceHandle":"{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-HWwQEœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"LangWatchEvaluatorComponent-MZkM1","targetHandle":"{œfieldNameœ:œanswerœ,œidœ:œLangWatchEvaluatorComponent-MZkM1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"answer","id":"LangWatchEvaluatorComponent-MZkM1","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatOutput","id":"ChatOutput-HWwQE","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatOutput-HWwQE{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-HWwQEœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-LangWatchEvaluatorComponent-MZkM1{œfieldNameœ:œanswerœ,œidœ:œLangWatchEvaluatorComponent-MZkM1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"ChatInput-0Q78M","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0Q78Mœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"LangWatchEvaluatorComponent-MZkM1","targetHandle":"{œfieldNameœ:œquestionœ,œidœ:œLangWatchEvaluatorComponent-MZkM1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"question","id":"LangWatchEvaluatorComponent-MZkM1","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-0Q78M","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-0Q78M{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0Q78Mœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-LangWatchEvaluatorComponent-MZkM1{œfieldNameœ:œquestionœ,œidœ:œLangWatchEvaluatorComponent-MZkM1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"NEO4J-c3EdX","sourceHandle":"{œdataTypeœ:œNEO4Jœ,œidœ:œNEO4J-c3EdXœ,œnameœ:œsearch_resultœ,œoutput_typesœ:[œDataœ]}","target":"LangWatchEvaluatorComponent-MZkM1","targetHandle":"{œfieldNameœ:œcontext_dataœ,œidœ:œLangWatchEvaluatorComponent-MZkM1œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"context_data","id":"LangWatchEvaluatorComponent-MZkM1","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"NEO4J","id":"NEO4J-c3EdX","name":"search_result","output_types":["Data"]}},"id":"reactflow__edge-NEO4J-c3EdX{œdataTypeœ:œNEO4Jœ,œidœ:œNEO4J-c3EdXœ,œnameœ:œsearch_resultœ,œoutput_typesœ:[œDataœ]}-LangWatchEvaluatorComponent-MZkM1{œfieldNameœ:œcontext_dataœ,œidœ:œLangWatchEvaluatorComponent-MZkM1œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":"","selected":false}],"viewport":{"x":675.2333264284516,"y":576.2759182620111,"zoom":0.48947275508301136}},"description":"Desafio 2 - Sistema de Perguntas e Respostas com RAG.","name":"Desafio 2 - QA com RAG","last_tested_version":"1.0.13","endpoint_name":null,"is_component":false}